{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LABORATORIO #7 ATAQUES A MODELOS \n",
    "##### Alfredo Quezada 191002\n",
    "##### Randy Venegas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Del laboratorio pasado, generamos un modelo y lo montamos para este laboratorio: \n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.layers import Conv2D, MaxPool2D, Dense, Flatten\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "from art.estimators.classification import KerasClassifier #No soporta TF 2\n",
    "from art.attacks.evasion import FastGradientMethod\n",
    "from art.utils import load_dataset\n",
    "\n",
    "# Disabling eager execution from TF 2\n",
    "tf.compat.v1.disable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "vulnerable_model = tf.keras.models.load_model(\"modelo7\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_images, train_labels), (test_images, test_labels), min, max = load_dataset(name=\"mnist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "clasificador = KerasClassifier(\n",
    "    model=vulnerable_model,\n",
    "    clip_values=(min, max))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ataque de Evasi칩n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ataque usando Gradiente Descendiente para generar imagenes adversariales\n",
    "#Epsilon define que tan fuerte ser치 el ataque:\n",
    "#Debe haber un equilibrio entre la fuerza del ataque y la detecci칩n a simple vista que permita detectar que una imagen fue atacada\n",
    "\n",
    "attack_fgsm = FastGradientMethod(\n",
    "    estimator=clasificador, \n",
    "    eps=0.3\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Quezada\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\engine\\training_v1.py:2359: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    }
   ],
   "source": [
    "#Generamos imagenes adversariales a partir de un dataset que queremos perturbar\n",
    "test_images_adv = attack_fgsm.generate(x=test_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1e6ff4e3220>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Ejemplo de una perturbaci칩n\n",
    "plt.imshow(X=test_images_adv[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "A target array with shape (10000, 10) was passed for an output of shape (None, 25) while using as loss `categorical_crossentropy`. This loss expects targets to have the same shape as the output.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Quezada\\Desktop\\uvg 2023\\Security Data Science\\Lab07-SDS\\LAB7.ipynb Cell 10\u001b[0m in \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Quezada/Desktop/uvg%202023/Security%20Data%20Science/Lab07-SDS/LAB7.ipynb#X12sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Evaluating the model on clean images\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Quezada/Desktop/uvg%202023/Security%20Data%20Science/Lab07-SDS/LAB7.ipynb#X12sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m score_clean \u001b[39m=\u001b[39m vulnerable_model\u001b[39m.\u001b[39;49mevaluate(\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Quezada/Desktop/uvg%202023/Security%20Data%20Science/Lab07-SDS/LAB7.ipynb#X12sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     x\u001b[39m=\u001b[39;49mtest_images, \n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Quezada/Desktop/uvg%202023/Security%20Data%20Science/Lab07-SDS/LAB7.ipynb#X12sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     y\u001b[39m=\u001b[39;49mtest_labels\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Quezada/Desktop/uvg%202023/Security%20Data%20Science/Lab07-SDS/LAB7.ipynb#X12sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     )\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Quezada/Desktop/uvg%202023/Security%20Data%20Science/Lab07-SDS/LAB7.ipynb#X12sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39m# Evaluating the model on adversarial images\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Quezada/Desktop/uvg%202023/Security%20Data%20Science/Lab07-SDS/LAB7.ipynb#X12sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m score_adv \u001b[39m=\u001b[39m vulnerable_model\u001b[39m.\u001b[39mevaluate(\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Quezada/Desktop/uvg%202023/Security%20Data%20Science/Lab07-SDS/LAB7.ipynb#X12sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     x\u001b[39m=\u001b[39mtest_images_adv, \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Quezada/Desktop/uvg%202023/Security%20Data%20Science/Lab07-SDS/LAB7.ipynb#X12sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     y\u001b[39m=\u001b[39mtest_labels\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Quezada/Desktop/uvg%202023/Security%20Data%20Science/Lab07-SDS/LAB7.ipynb#X12sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\engine\\training_v1.py:975\u001b[0m, in \u001b[0;36mModel.evaluate\u001b[1;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m    972\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_call_args(\u001b[39m\"\u001b[39m\u001b[39mevaluate\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    974\u001b[0m func \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_training_loop(x)\n\u001b[1;32m--> 975\u001b[0m \u001b[39mreturn\u001b[39;00m func\u001b[39m.\u001b[39;49mevaluate(\n\u001b[0;32m    976\u001b[0m     \u001b[39mself\u001b[39;49m,\n\u001b[0;32m    977\u001b[0m     x\u001b[39m=\u001b[39;49mx,\n\u001b[0;32m    978\u001b[0m     y\u001b[39m=\u001b[39;49my,\n\u001b[0;32m    979\u001b[0m     batch_size\u001b[39m=\u001b[39;49mbatch_size,\n\u001b[0;32m    980\u001b[0m     verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[0;32m    981\u001b[0m     sample_weight\u001b[39m=\u001b[39;49msample_weight,\n\u001b[0;32m    982\u001b[0m     steps\u001b[39m=\u001b[39;49msteps,\n\u001b[0;32m    983\u001b[0m     callbacks\u001b[39m=\u001b[39;49mcallbacks,\n\u001b[0;32m    984\u001b[0m     max_queue_size\u001b[39m=\u001b[39;49mmax_queue_size,\n\u001b[0;32m    985\u001b[0m     workers\u001b[39m=\u001b[39;49mworkers,\n\u001b[0;32m    986\u001b[0m     use_multiprocessing\u001b[39m=\u001b[39;49muse_multiprocessing,\n\u001b[0;32m    987\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\engine\\training_arrays_v1.py:767\u001b[0m, in \u001b[0;36mArrayLikeTrainingLoop.evaluate\u001b[1;34m(self, model, x, y, batch_size, verbose, sample_weight, steps, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m    754\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mevaluate\u001b[39m(\n\u001b[0;32m    755\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    756\u001b[0m     model,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    764\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m    765\u001b[0m ):\n\u001b[0;32m    766\u001b[0m     batch_size \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39m_validate_or_infer_batch_size(batch_size, steps, x)\n\u001b[1;32m--> 767\u001b[0m     x, y, sample_weights \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49m_standardize_user_data(\n\u001b[0;32m    768\u001b[0m         x,\n\u001b[0;32m    769\u001b[0m         y,\n\u001b[0;32m    770\u001b[0m         sample_weight\u001b[39m=\u001b[39;49msample_weight,\n\u001b[0;32m    771\u001b[0m         batch_size\u001b[39m=\u001b[39;49mbatch_size,\n\u001b[0;32m    772\u001b[0m         check_steps\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m    773\u001b[0m         steps_name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39msteps\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    774\u001b[0m         steps\u001b[39m=\u001b[39;49msteps,\n\u001b[0;32m    775\u001b[0m     )\n\u001b[0;32m    776\u001b[0m     \u001b[39mreturn\u001b[39;00m test_loop(\n\u001b[0;32m    777\u001b[0m         model,\n\u001b[0;32m    778\u001b[0m         inputs\u001b[39m=\u001b[39mx,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    784\u001b[0m         callbacks\u001b[39m=\u001b[39mcallbacks,\n\u001b[0;32m    785\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\engine\\training_v1.py:2652\u001b[0m, in \u001b[0;36mModel._standardize_user_data\u001b[1;34m(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split, shuffle, extract_tensors_from_dataset)\u001b[0m\n\u001b[0;32m   2643\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[0;32m   2644\u001b[0m     \u001b[39mnot\u001b[39;00m run_eagerly\n\u001b[0;32m   2645\u001b[0m     \u001b[39mand\u001b[39;00m is_build_called\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2648\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39many\u001b[39m(_is_symbolic_tensor(v) \u001b[39mfor\u001b[39;00m v \u001b[39min\u001b[39;00m all_inputs)\n\u001b[0;32m   2649\u001b[0m ):\n\u001b[0;32m   2650\u001b[0m     \u001b[39mreturn\u001b[39;00m [], [], \u001b[39mNone\u001b[39;00m\n\u001b[1;32m-> 2652\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_standardize_tensors(\n\u001b[0;32m   2653\u001b[0m     x,\n\u001b[0;32m   2654\u001b[0m     y,\n\u001b[0;32m   2655\u001b[0m     sample_weight,\n\u001b[0;32m   2656\u001b[0m     run_eagerly\u001b[39m=\u001b[39;49mrun_eagerly,\n\u001b[0;32m   2657\u001b[0m     dict_inputs\u001b[39m=\u001b[39;49mdict_inputs,\n\u001b[0;32m   2658\u001b[0m     is_dataset\u001b[39m=\u001b[39;49mis_dataset,\n\u001b[0;32m   2659\u001b[0m     class_weight\u001b[39m=\u001b[39;49mclass_weight,\n\u001b[0;32m   2660\u001b[0m     batch_size\u001b[39m=\u001b[39;49mbatch_size,\n\u001b[0;32m   2661\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\engine\\training_v1.py:2783\u001b[0m, in \u001b[0;36mModel._standardize_tensors\u001b[1;34m(self, x, y, sample_weight, run_eagerly, dict_inputs, is_dataset, class_weight, batch_size)\u001b[0m\n\u001b[0;32m   2779\u001b[0m         training_utils_v1\u001b[39m.\u001b[39mcheck_array_lengths(x, y, sample_weights)\n\u001b[0;32m   2780\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_is_graph_network \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m run_eagerly:\n\u001b[0;32m   2781\u001b[0m             \u001b[39m# Additional checks to avoid users mistakenly using improper\u001b[39;00m\n\u001b[0;32m   2782\u001b[0m             \u001b[39m# loss fns.\u001b[39;00m\n\u001b[1;32m-> 2783\u001b[0m             training_utils_v1\u001b[39m.\u001b[39;49mcheck_loss_and_target_compatibility(\n\u001b[0;32m   2784\u001b[0m                 y, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_feed_loss_fns, feed_output_shapes\n\u001b[0;32m   2785\u001b[0m             )\n\u001b[0;32m   2787\u001b[0m     sample_weights, _, _ \u001b[39m=\u001b[39m training_utils\u001b[39m.\u001b[39mhandle_partial_sample_weights(\n\u001b[0;32m   2788\u001b[0m         y, sample_weights, feed_sample_weight_modes, check_all_flat\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m\n\u001b[0;32m   2789\u001b[0m     )\n\u001b[0;32m   2790\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\engine\\training_utils_v1.py:945\u001b[0m, in \u001b[0;36mcheck_loss_and_target_compatibility\u001b[1;34m(targets, loss_fns, output_shapes)\u001b[0m\n\u001b[0;32m    943\u001b[0m     loss_type \u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mfn \u001b[39mif\u001b[39;00m is_loss_wrapper \u001b[39melse\u001b[39;00m \u001b[39mtype\u001b[39m(loss)\n\u001b[0;32m    944\u001b[0m     loss_name \u001b[39m=\u001b[39m loss_type\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\n\u001b[1;32m--> 945\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    946\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mA target array with shape \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    947\u001b[0m     \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(y\u001b[39m.\u001b[39mshape)\n\u001b[0;32m    948\u001b[0m     \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m was passed for an output of shape \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    949\u001b[0m     \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(shape)\n\u001b[0;32m    950\u001b[0m     \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m while using as loss `\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    951\u001b[0m     \u001b[39m+\u001b[39m loss_name\n\u001b[0;32m    952\u001b[0m     \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m`. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    953\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mThis loss expects targets to have the same shape \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    954\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mas the output.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    955\u001b[0m )\n",
      "\u001b[1;31mValueError\u001b[0m: A target array with shape (10000, 10) was passed for an output of shape (None, 25) while using as loss `categorical_crossentropy`. This loss expects targets to have the same shape as the output."
     ]
    }
   ],
   "source": [
    "# Evaluating the model on clean images\n",
    "score_clean = vulnerable_model.evaluate(\n",
    "    x=test_images, \n",
    "    y=test_labels\n",
    "    )\n",
    "\n",
    "# Evaluating the model on adversarial images\n",
    "score_adv = vulnerable_model.evaluate(\n",
    "    x=test_images_adv, \n",
    "    y=test_labels\n",
    "    )\n",
    "\n",
    "# Comparing test losses\n",
    "print(f\"Clean test set loss: {score_clean[0]:.2f} \" \n",
    "      f\"vs adversarial set test loss: {score_adv[0]:.2f}\")\n",
    "\n",
    "# Comparing test accuracies\n",
    "print(f\"Clean test set accuracy: {score_clean[1]:.2f} \" \n",
    "      f\"vs adversarial test set accuracy: {score_adv[1]:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Se prueba el accuracy para distintos valores de epsilon\n",
    "\n",
    "# Setting the number of rows and columns for the figure\n",
    "nrows, ncols = 2, 5\n",
    "\n",
    "# Generating subplots\n",
    "fig, axes = plt.subplots(\n",
    "    nrows=nrows, \n",
    "    ncols=ncols, \n",
    "    figsize=(20, 10)\n",
    "    )\n",
    "\n",
    "# Defining a range of eps values to try\n",
    "eps_to_try = [0.01, 0.025, 0.05, 0.075, 0.1, 0.125, 0.15, 0.175, 0.2, 0.25]\n",
    "\n",
    "# Defining a counting variable to traverse eps_to_try\n",
    "counter = 0\n",
    "\n",
    "# Iterating over rows and cols\n",
    "for i in range(nrows):\n",
    "    for j in range(ncols):    \n",
    "        # Creating an attack object for the current value of eps    \n",
    "        attack_fgsm = FastGradientMethod(\n",
    "            estimator=clasificador, \n",
    "            eps=eps_to_try[counter]\n",
    "            )\n",
    "\n",
    "        # Generating adversarial images\n",
    "        test_images_adv = attack_fgsm.generate(x=test_images)\n",
    "\n",
    "        # Showing the first adversarial image\n",
    "        axes[i, j].imshow(X=test_images_adv[0])\n",
    "\n",
    "        # Disabling x and y ticks\n",
    "        axes[i, j].set_xticks(ticks=[])\n",
    "        axes[i, j].set_yticks(ticks=[])\n",
    "\n",
    "        # Evaluating model performance on adversarial samples and retrieving test accuracy\n",
    "        test_score = clasificador._model.evaluate(\n",
    "            x=test_images_adv, \n",
    "            y=test_labels\n",
    "            )[1]\n",
    "\n",
    "        # Getting prediction for the image that we displayed\n",
    "        prediction = np.argmax(vulnerable_model.predict(\n",
    "            x=np.expand_dims(a=test_images_adv[0], \n",
    "            axis=0)\n",
    "            ))    \n",
    "\n",
    "        # Showing the current eps value, test accuracy, and prediction\n",
    "        axes[i, j].set_title(\n",
    "            label=f\"Eps value: {eps_to_try[counter]}\\n\"\n",
    "            f\"Test accuracy: {test_score * 100:.2f}%\\n\"\n",
    "            f\"Prediction: {prediction}\"\n",
    "            )\n",
    "\n",
    "        # Incrementing counter\n",
    "        counter += 1\n",
    "\n",
    "# Showing the plot\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
