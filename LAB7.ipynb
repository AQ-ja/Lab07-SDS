{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LABORATORIO #7 ATAQUES A MODELOS \n",
    "##### Alfredo Quezada 191002\n",
    "##### Randy Venegas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Quezada\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "C:\\Users\\Quezada\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\art\\estimators\\certification\\__init__.py:14: UserWarning: PyTorch not found. Not importing DeepZ or Interval Bound Propagation functionality\n",
      "  warnings.warn(\"PyTorch not found. Not importing DeepZ or Interval Bound Propagation functionality\")\n"
     ]
    }
   ],
   "source": [
    "# Del laboratorio pasado, generamos un modelo y lo montamos para este laboratorio: \n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.layers import Conv2D, MaxPool2D, Dense, Flatten\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "from art.estimators.classification import KerasClassifier #No soporta TF 2\n",
    "from art.attacks.evasion import FastGradientMethod\n",
    "from art.utils import load_dataset\n",
    "\n",
    "# Disabling eager execution from TF 2\n",
    "tf.compat.v1.disable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "vulnerable_model = tf.keras.models.load_model(\"modelo7\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_images, train_labels), (test_images, test_labels), min, max = load_dataset(name=\"mnist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "clasificador = KerasClassifier(\n",
    "    model=vulnerable_model,\n",
    "    clip_values=(min, max))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ataque de Evasión"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ataque usando Gradiente Descendiente para generar imagenes adversariales\n",
    "#Epsilon define que tan fuerte será el ataque:\n",
    "#Debe haber un equilibrio entre la fuerza del ataque y la detección a simple vista que permita detectar que una imagen fue atacada\n",
    "\n",
    "attack_fgsm = FastGradientMethod(\n",
    "    estimator=clasificador, \n",
    "    eps=0.3\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking input: expected conv2d_12_input to have shape (64, 64, 3) but got array with shape (28, 28, 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Quezada\\Desktop\\uvg 2023\\Security Data Science\\Lab07-SDS\\LAB7.ipynb Cell 8\u001b[0m in \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Quezada/Desktop/uvg%202023/Security%20Data%20Science/Lab07-SDS/LAB7.ipynb#X10sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m#Generamos imagenes adversariales a partir de un dataset que queremos perturbar\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Quezada/Desktop/uvg%202023/Security%20Data%20Science/Lab07-SDS/LAB7.ipynb#X10sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m test_images_adv \u001b[39m=\u001b[39m attack_fgsm\u001b[39m.\u001b[39;49mgenerate(x\u001b[39m=\u001b[39;49mtest_images)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\art\\attacks\\evasion\\fast_gradient.py:237\u001b[0m, in \u001b[0;36mFastGradientMethod.generate\u001b[1;34m(self, x, y, **kwargs)\u001b[0m\n\u001b[0;32m    235\u001b[0m     \u001b[39m# Use model predictions as correct outputs\u001b[39;00m\n\u001b[0;32m    236\u001b[0m     logger\u001b[39m.\u001b[39minfo(\u001b[39m\"\u001b[39m\u001b[39mUsing model predictions as correct labels for FGM.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 237\u001b[0m     y_array \u001b[39m=\u001b[39m get_labels_np_array(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mestimator\u001b[39m.\u001b[39;49mpredict(x, batch_size\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbatch_size))  \u001b[39m# type: ignore\u001b[39;00m\n\u001b[0;32m    238\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    239\u001b[0m     y_array \u001b[39m=\u001b[39m y\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\art\\estimators\\classification\\classifier.py:73\u001b[0m, in \u001b[0;36mInputFilter.__init__.<locals>.make_replacement.<locals>.replacement_function\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     71\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m     72\u001b[0m     args \u001b[39m=\u001b[39m \u001b[39mtuple\u001b[39m(lst)\n\u001b[1;32m---> 73\u001b[0m \u001b[39mreturn\u001b[39;00m fdict[func_name](\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\art\\estimators\\classification\\keras.py:551\u001b[0m, in \u001b[0;36mKerasClassifier.predict\u001b[1;34m(self, x, batch_size, training_mode, **kwargs)\u001b[0m\n\u001b[0;32m    549\u001b[0m     predictions \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_model(x_preprocessed, training\u001b[39m=\u001b[39mtraining_mode)\n\u001b[0;32m    550\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 551\u001b[0m     predictions \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_model\u001b[39m.\u001b[39;49mpredict(x_preprocessed, batch_size\u001b[39m=\u001b[39;49mbatch_size)\n\u001b[0;32m    553\u001b[0m \u001b[39m# Apply postprocessing\u001b[39;00m\n\u001b[0;32m    554\u001b[0m predictions \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_apply_postprocessing(preds\u001b[39m=\u001b[39mpredictions, fit\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\engine\\training_v1.py:1059\u001b[0m, in \u001b[0;36mModel.predict\u001b[1;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1056\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_call_args(\u001b[39m\"\u001b[39m\u001b[39mpredict\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m   1058\u001b[0m func \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_training_loop(x)\n\u001b[1;32m-> 1059\u001b[0m \u001b[39mreturn\u001b[39;00m func\u001b[39m.\u001b[39;49mpredict(\n\u001b[0;32m   1060\u001b[0m     \u001b[39mself\u001b[39;49m,\n\u001b[0;32m   1061\u001b[0m     x\u001b[39m=\u001b[39;49mx,\n\u001b[0;32m   1062\u001b[0m     batch_size\u001b[39m=\u001b[39;49mbatch_size,\n\u001b[0;32m   1063\u001b[0m     verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[0;32m   1064\u001b[0m     steps\u001b[39m=\u001b[39;49msteps,\n\u001b[0;32m   1065\u001b[0m     callbacks\u001b[39m=\u001b[39;49mcallbacks,\n\u001b[0;32m   1066\u001b[0m     max_queue_size\u001b[39m=\u001b[39;49mmax_queue_size,\n\u001b[0;32m   1067\u001b[0m     workers\u001b[39m=\u001b[39;49mworkers,\n\u001b[0;32m   1068\u001b[0m     use_multiprocessing\u001b[39m=\u001b[39;49muse_multiprocessing,\n\u001b[0;32m   1069\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\engine\\training_arrays_v1.py:798\u001b[0m, in \u001b[0;36mArrayLikeTrainingLoop.predict\u001b[1;34m(self, model, x, batch_size, verbose, steps, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m    787\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpredict\u001b[39m(\n\u001b[0;32m    788\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    789\u001b[0m     model,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    795\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m    796\u001b[0m ):\n\u001b[0;32m    797\u001b[0m     batch_size \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39m_validate_or_infer_batch_size(batch_size, steps, x)\n\u001b[1;32m--> 798\u001b[0m     x, _, _ \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49m_standardize_user_data(\n\u001b[0;32m    799\u001b[0m         x, check_steps\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, steps_name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39msteps\u001b[39;49m\u001b[39m\"\u001b[39;49m, steps\u001b[39m=\u001b[39;49msteps\n\u001b[0;32m    800\u001b[0m     )\n\u001b[0;32m    801\u001b[0m     \u001b[39mreturn\u001b[39;00m predict_loop(\n\u001b[0;32m    802\u001b[0m         model,\n\u001b[0;32m    803\u001b[0m         x,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    807\u001b[0m         callbacks\u001b[39m=\u001b[39mcallbacks,\n\u001b[0;32m    808\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\engine\\training_v1.py:2652\u001b[0m, in \u001b[0;36mModel._standardize_user_data\u001b[1;34m(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split, shuffle, extract_tensors_from_dataset)\u001b[0m\n\u001b[0;32m   2643\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[0;32m   2644\u001b[0m     \u001b[39mnot\u001b[39;00m run_eagerly\n\u001b[0;32m   2645\u001b[0m     \u001b[39mand\u001b[39;00m is_build_called\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2648\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39many\u001b[39m(_is_symbolic_tensor(v) \u001b[39mfor\u001b[39;00m v \u001b[39min\u001b[39;00m all_inputs)\n\u001b[0;32m   2649\u001b[0m ):\n\u001b[0;32m   2650\u001b[0m     \u001b[39mreturn\u001b[39;00m [], [], \u001b[39mNone\u001b[39;00m\n\u001b[1;32m-> 2652\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_standardize_tensors(\n\u001b[0;32m   2653\u001b[0m     x,\n\u001b[0;32m   2654\u001b[0m     y,\n\u001b[0;32m   2655\u001b[0m     sample_weight,\n\u001b[0;32m   2656\u001b[0m     run_eagerly\u001b[39m=\u001b[39;49mrun_eagerly,\n\u001b[0;32m   2657\u001b[0m     dict_inputs\u001b[39m=\u001b[39;49mdict_inputs,\n\u001b[0;32m   2658\u001b[0m     is_dataset\u001b[39m=\u001b[39;49mis_dataset,\n\u001b[0;32m   2659\u001b[0m     class_weight\u001b[39m=\u001b[39;49mclass_weight,\n\u001b[0;32m   2660\u001b[0m     batch_size\u001b[39m=\u001b[39;49mbatch_size,\n\u001b[0;32m   2661\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\engine\\training_v1.py:2693\u001b[0m, in \u001b[0;36mModel._standardize_tensors\u001b[1;34m(self, x, y, sample_weight, run_eagerly, dict_inputs, is_dataset, class_weight, batch_size)\u001b[0m\n\u001b[0;32m   2690\u001b[0m \u001b[39m# Standardize the inputs.\u001b[39;00m\n\u001b[0;32m   2691\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(x, (tf\u001b[39m.\u001b[39mcompat\u001b[39m.\u001b[39mv1\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mDataset, tf\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mDataset)):\n\u001b[0;32m   2692\u001b[0m     \u001b[39m# TODO(fchollet): run static checks with dataset output shape(s).\u001b[39;00m\n\u001b[1;32m-> 2693\u001b[0m     x \u001b[39m=\u001b[39m training_utils_v1\u001b[39m.\u001b[39;49mstandardize_input_data(\n\u001b[0;32m   2694\u001b[0m         x,\n\u001b[0;32m   2695\u001b[0m         feed_input_names,\n\u001b[0;32m   2696\u001b[0m         feed_input_shapes,\n\u001b[0;32m   2697\u001b[0m         check_batch_axis\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,  \u001b[39m# Don't enforce the batch size.\u001b[39;49;00m\n\u001b[0;32m   2698\u001b[0m         exception_prefix\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39minput\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m   2699\u001b[0m     )\n\u001b[0;32m   2701\u001b[0m \u001b[39m# Get typespecs for the input data and sanitize it if necessary.\u001b[39;00m\n\u001b[0;32m   2702\u001b[0m \u001b[39m# TODO(momernick): This should be capable of doing full input validation\u001b[39;00m\n\u001b[0;32m   2703\u001b[0m \u001b[39m# at all times - validate that this is so and refactor the\u001b[39;00m\n\u001b[0;32m   2704\u001b[0m \u001b[39m# standardization code.\u001b[39;00m\n\u001b[0;32m   2705\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(x, tf\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mDataset):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\engine\\training_utils_v1.py:731\u001b[0m, in \u001b[0;36mstandardize_input_data\u001b[1;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[0;32m    725\u001b[0m             \u001b[39mfor\u001b[39;00m dim, ref_dim \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(data_shape, shape):\n\u001b[0;32m    726\u001b[0m                 \u001b[39mif\u001b[39;00m (\n\u001b[0;32m    727\u001b[0m                     ref_dim \u001b[39m!=\u001b[39m dim\n\u001b[0;32m    728\u001b[0m                     \u001b[39mand\u001b[39;00m ref_dim \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    729\u001b[0m                     \u001b[39mand\u001b[39;00m dim \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    730\u001b[0m                 ):\n\u001b[1;32m--> 731\u001b[0m                     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    732\u001b[0m                         \u001b[39m\"\u001b[39m\u001b[39mError when checking \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    733\u001b[0m                         \u001b[39m+\u001b[39m exception_prefix\n\u001b[0;32m    734\u001b[0m                         \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m: expected \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    735\u001b[0m                         \u001b[39m+\u001b[39m names[i]\n\u001b[0;32m    736\u001b[0m                         \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m to have shape \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    737\u001b[0m                         \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(shape)\n\u001b[0;32m    738\u001b[0m                         \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m but got array with shape \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    739\u001b[0m                         \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(data_shape)\n\u001b[0;32m    740\u001b[0m                     )\n\u001b[0;32m    741\u001b[0m \u001b[39mreturn\u001b[39;00m data\n",
      "\u001b[1;31mValueError\u001b[0m: Error when checking input: expected conv2d_12_input to have shape (64, 64, 3) but got array with shape (28, 28, 1)"
     ]
    }
   ],
   "source": [
    "#Generamos imagenes adversariales a partir de un dataset que queremos perturbar\n",
    "test_images_adv = attack_fgsm.generate(x=test_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ejemplo de una perturbación\n",
    "plt.imshow(X=test_images_adv[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
